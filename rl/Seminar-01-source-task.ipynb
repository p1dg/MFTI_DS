{"cells":[{"cell_type":"markdown","metadata":{"id":"XkG_ZcvCuQJ0"},"source":["# Машинное обучение с подкреплением. Сбербанк осень 2020.\n","## Семинар 1\n","### Обучение с подкреплением: OpenAI gym, CrossEntropy Method (CEM), Deep CEM."]},{"cell_type":"markdown","metadata":{"id":"d2_vJZIY64of"},"source":["Импортируем необходимые библиотеки и настраиваем визуализацию:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"iG2GFu-j64of"},"outputs":[],"source":["try:\n","    import colab\n","    COLAB = True\n","except ModuleNotFoundError:\n","    COLAB = False\n","    pass\n","\n","if COLAB:\n","    !wget https://gist.githubusercontent.com/Tviskaron/4d35eabce2e057dd2ea49a00b00aaa41/raw/f1e25fc6ac6d8f11cb585559ce8b2ab9ffefd67b/colab_render.sh -O colab_render.sh -q\n","    !sh colab_render.sh\n","    !wget https://gist.githubusercontent.com/Tviskaron/d91decc1ca5f1b09af2f9f080011a925/raw/0d3474f65b4aea533996ee00edf99a37e4da5561/colab_render.py -O colab_render.py -q \n","    import colab_render"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"cYkSn3Tv64of"},"outputs":[{"ename":"ImportError","evalue":"\n    Error occurred while running `from pyglet.gl import *`\n    HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python-opengl'.\n    If you're running on a server, you may need a virtual frame buffer; something like this should work:\n    'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'\n    ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/__init__.py:95\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"OpenGL and GLU interface.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mThis package imports all OpenGL, GLU and registered OpenGL extension\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mbelow.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GLException\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/lib.py:149\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib_glx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m link_GL, link_GLU, link_GLX\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/gl/lib_glx.py:46\u001b[0m\n\u001b[1;32m     45\u001b[0m gl_lib \u001b[38;5;241m=\u001b[39m pyglet\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mload_library(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m glu_lib \u001b[38;5;241m=\u001b[39m \u001b[43mpyglet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGLU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Look for glXGetProcAddressARB extension, use it as fallback (for ATI fglrx and DRI drivers).\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyglet/lib.py:164\u001b[0m, in \u001b[0;36mLibraryLoader.load_library\u001b[0;34m(self, *names, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error loading library \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, \u001b[38;5;28mstr\u001b[39m(o)))\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLibrary \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m names[\u001b[38;5;241m0\u001b[39m])\n","\u001b[0;31mImportError\u001b[0m: Library \"GLU\" not found.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;28;01mas\u001b[39;00m ipythondisplay\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rendering\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Error occurred while running `from pyglet.gl import *`\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python-opengl'.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    If you're running on a server, you may need a virtual frame buffer; something like this should work:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: \n    Error occurred while running `from pyglet.gl import *`\n    HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python-opengl'.\n    If you're running on a server, you may need a virtual frame buffer; something like this should work:\n    'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'\n    "]}],"source":["# библиотеки и функции, которые потребуеются для показа видео\n","\n","import glob\n","import io\n","import base64\n","from IPython import display as ipythondisplay\n","from IPython.display import HTML\n","from gym.envs.classic_control import rendering\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","\n","org_constructor = rendering.Viewer.__init__\n","\n","\n","def constructor(self, *args, **kwargs):\n","    org_constructor(self, *args, **kwargs)\n","    self.window.set_visible(visible=False)\n","\n","\n","rendering.Viewer.__init__ = constructor\n","\n","\n","def show_video(folder=\"./video\"):\n","    mp4list = glob.glob(folder + '/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")"]},{"cell_type":"markdown","metadata":{"id":"p1DxL2BU64of"},"source":["## 1. OpenAI Gym"]},{"cell_type":"markdown","metadata":{"id":"WgC3duS064of"},"source":["[Gym](https://gym.openai.com) $-$ это набор инструментов для разработки и сравнения алгоритмов обучения с подкреплением, который также включает в себя большой [набор окружений](https://gym.openai.com/envs/)."]},{"cell_type":"markdown","metadata":{"id":"XCJGBafN64of"},"source":["### Создание окружения\n","\n","Для создания окружения используется функция ```gym.make(<имя окружения>)```. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Y3pp6hP64of"},"outputs":[{"name":"stdout","output_type":"stream","text":["state: [-0.49467188  0.        ]\n","next_state: [-0.49588856 -0.00121668] , r: -1.0, done: False, info: {}\n"]}],"source":["import gym\n","\n","# Создаем окружение\n","env = gym.make(\"MountainCar-v0\")\n","\n","# Инициализируем окружение\n","state = env.reset()\n","print(f\"state: {state}\")\n","\n","# Выполняем действие в среде \n","next_state, r, done, info = env.step(0)\n","print(f\"next_state: {next_state} , r: {r}, done: {done}, info: {info}\")\n","\n","# Закрываем окружение\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"_aKLon2F64of"},"source":["### Основные методы окружения:\n","\n","* ``reset()`` $-$ инициализация окружения, возвращает первое наблюдение (состояние).  \n","* ``step(a)`` $-$ выполнить в среде действие $\\mathbf{a}$ и получить кортеж: $\\mathbf{\\langle s_{t+1}, r_t, done, info \\rangle}$, где $\\mathbf{s_{t+1}}$ - следующее состояние, $\\mathbf{r_t}$ - вознаграждение, $\\mathbf{done}$ - флаг заверешния, $\\mathbf{info}$ - дополнительная информация\n","\n","### Дополнительные методы:\n","* ``render()`` $-$ визуализация текущего состояния среды (удобно, если мы запускаем локально, в колабе ничего не увидим)\n","\n","* ``close()`` $-$ закрывает окружение "]},{"cell_type":"markdown","metadata":{"id":"ZCbYP-zP64of"},"source":["### Свойства среды:\n","* ``env.observation_space`` $-$ информация о пространстве состояний\n","* ``env.action_space`` $-$ информация о пространстве действий\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWH53-rE64of"},"outputs":[{"name":"stdout","output_type":"stream","text":["env.observation_space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n","env.action_space: Discrete(3)\n"]}],"source":["print(f\"env.observation_space: {env.observation_space}\")\n","print(f\"env.action_space: {env.action_space}\")"]},{"cell_type":"markdown","metadata":{"id":"uASEWtKU64of"},"source":["### Среда ``MountainCar-v0``\n","\n","Информацию о любой среде можно найти в [исходниках](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) или на [сайте](https://gym.openai.com/envs/MountainCar-v0/). О ``MountainCar-v0`` мы можем узнать следующее: \n","\n","#### Задание:\n","Автомобиль едет по одномерному треку между двумя холмами. Цель состоит в том, чтобы заехать на правый холм; однако двигатель машины недостаточно мощный, чтобы взобраться на холм за один проход. Следовательно, единственный способ добиться успеха $-$ это двигаться вперед и назад, чтобы набрать обороты.\n","\n","#### Пространство состояний Box(2):\n","\n","\n","\n","Num | Observation  | Min  | Max  \n","----|--------------|------|----   \n","0   | position     | -1.2 | 0.6\n","1   | velocity     | -0.07| 0.07\n","\n","\n","#### Пространство действий Discrete(3):\n","\n","\n","\n","Num | Action|\n","----|-------------|\n","0   | push left   |\n","1   | no push     |\n","2   | push right  |\n","\n","* Вознаграждения: -1 за каждый шаг, пока не достигнута цель \n","\n","* Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."]},{"cell_type":"markdown","metadata":{"id":"aUicAEOB64of"},"source":["### Пример со случайной стратегией:\n","\n","Для выбора действия используется ``env.action_space.sample()``"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1Jc6nAU64of"},"outputs":[{"ename":"NameError","evalue":"name 'glPushMatrix' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mMonitor(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./video\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# проводим инициализацию и запоминаем начальное состояние\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# выполняем действие, получаем s, r, done, info\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/monitor.py:56\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_reset()\n\u001b[1;32m     55\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/monitor.py:241\u001b[0m, in \u001b[0;36mMonitor._after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Reset the stat count\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_recorder\u001b[38;5;241m.\u001b[39mafter_reset(observation)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Bump *after* all reset activity has finished\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/monitor.py:267\u001b[0m, in \u001b[0;36mMonitor.reset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Start recording the next video.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# TODO: calculate a more correct 'episode_id' upon merge\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;241m=\u001b[39m video_recorder\u001b[38;5;241m.\u001b[39mVideoRecorder(\n\u001b[1;32m    257\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[1;32m    258\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_enabled(),\n\u001b[1;32m    266\u001b[0m )\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:132\u001b[0m, in \u001b[0;36mVideoRecorder.capture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapturing video frame: path=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    131\u001b[0m render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mansi_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 132\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/mountain_car.py:168\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcartrans\u001b[38;5;241m.\u001b[39mset_translation(\n\u001b[1;32m    164\u001b[0m     (pos \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_position) \u001b[38;5;241m*\u001b[39m scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_height(pos) \u001b[38;5;241m*\u001b[39m scale\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcartrans\u001b[38;5;241m.\u001b[39mset_rotation(math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m pos))\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:126\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mdispatch_events()\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m geom \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeoms:\n\u001b[1;32m    128\u001b[0m     geom\u001b[38;5;241m.\u001b[39mrender()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:232\u001b[0m, in \u001b[0;36mTransform.enable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 232\u001b[0m     \u001b[43mglPushMatrix\u001b[49m()\n\u001b[1;32m    233\u001b[0m     glTranslatef(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    235\u001b[0m     )  \u001b[38;5;66;03m# translate to GL loc ppint\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     glRotatef(RAD2DEG \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'glPushMatrix' is not defined"]}],"source":["# создаем окружение\n","env = gym.make(\"MountainCar-v0\")\n","# добавляем wrapper (обертку), чтобы задать ограничение на число шагов в среде\n","env = gym.wrappers.TimeLimit(env, max_episode_steps=250)\n","# # добавляем визуализацию\n","env = gym.wrappers.Monitor(env, \"./video\", force=True)\n","\n","# проводим инициализацию и запоминаем начальное состояние\n","s = env.reset()\n","done = False\n","\n","while not done:\n","    # выполняем действие, получаем s, r, done, info\n","    s, r, done, _ = env.step(env.action_space.sample())\n","    \n","env.close()\n","\n","# Сначала закрываем окружение, чтобы видео записалось полностью\n","env.close()\n","show_video()"]},{"cell_type":"markdown","metadata":{"id":"ZYVJ4YYD64of"},"source":["### Задание 1:\n","В среде MountainCar-v0 мы хотим, чтобы машина достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции act() ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCxsTEln64of"},"outputs":[],"source":["def act(s):\n","    # список возможных действий\n","    left, stop, right = 0, 1, 2\n","    \n","    # позиция и скорость\n","    position, velocity = s\n","    # пример: можем попробовать ехать только влево\n","    # action = actions['left']\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    return action"]},{"cell_type":"markdown","metadata":{"id":"rQy22ZiK64of"},"source":["Проверяем решение:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yAO7lCd64of"},"outputs":[],"source":["env = gym.wrappers.TimeLimit(gym.make(\"MountainCar-v0\"), max_episode_steps=250)\n","env = gym.wrappers.Monitor(env, \"./video\", force=True)\n","\n","# проводим инициализацию и запоминаем начальное состояние\n","s = env.reset()\n","done = False\n","\n","while not done:\n","    # выполняем действие, получаем s, r, done, info\n","    s, r, done, _ = env.step(act(s))\n","    \n","if s[0] > 0.47:\n","    print(\"Принято!\")\n","else:\n","    print(\"\"\"Исправьте функцию выбора действия!\"\"\")\n","\n","\n","env.close()\n","show_video()"]},{"cell_type":"markdown","metadata":{"id":"UM7vcYORuQJ1"},"source":["## 2. Crossentropy Method\n","\n","В этой пункте мы посмотрим на то, как решить задачи RL с помощью метода crossentropy.\n","\n","Рассмотрим пример с задачей Taxi [Dietterich, 2000]. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5yU3RZuuQJ2"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import gym\n","\n","env = gym.make(\"Taxi-v3\")\n","env.reset()\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xyi1DDjQuQJ8"},"outputs":[],"source":["n_states  = env.observation_space.n\n","n_actions = env.action_space.n  \n","\n","print(f\"состояний: {n_states} действий: {n_actions}\")"]},{"cell_type":"markdown","metadata":{"id":"hR-L-rHquQKA"},"source":["В этот раз нашей стратегией будет вероятностной распределение. \n","\n","$\\pi(s,a) = P(a|s)$\n","\n","Для задачи такси мы можем использовать таблицу: \n","\n","policy[s,a] = P(выбрать действие a | в состоянии s)\n","\n","Создадим \"равномерную\" стратегию в виде двумерного массива с \n","равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVBlXo-huQKB"},"outputs":[],"source":["def initialize_policy(n_states, n_actions):\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    return policy\n","\n","policy = initialize_policy(n_states, n_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xqVJHT4uQKE"},"outputs":[],"source":["assert type(policy) in (np.ndarray, np.matrix)\n","assert np.allclose(policy, 1./n_actions)\n","assert np.allclose(np.sum(policy, axis=1), 1)"]},{"cell_type":"markdown","metadata":{"id":"iDx_ek7vuQKH"},"source":["### Генерация сессий взаимодейтсвия со средой.\n","\n","Мы будем запоминать все состояния, действия и вознаграждения за эпизод."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlsAeF7EuQKI"},"outputs":[],"source":["def generate_session(env, policy, t_max=10**4):\n","    \"\"\"\n","    Игра идет до конца эпизода или до t_max шагов в окружении. \n","    :param policy: [n_states,n_actions] \n","    :returns: states - список состояний, actions - список действий, total_reward - итоговое вознаграждение\n","    \"\"\"\n","    states, actions = [], []\n","    total_reward = 0.\n","\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # Подсказка: вы можете использовать np.random.choice для сэмплирования\n","        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n","        # a = \n","        ####### Здесь ваш код ########\n","        raise NotImplementedError\n","        ##############################\n","        new_s, r, done, info = env.step(a)\n","\n","        # Record information we just got from the environment.\n","        states.append(s)\n","        actions.append(a)\n","        total_reward += r\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, total_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HM2-QI3UuQKL"},"outputs":[],"source":["s, a, r = generate_session(env, policy)\n","assert type(s) == type(a) == list\n","assert len(s) == len(a)\n","assert type(r) in [float, np.float]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWqyUSLJuQKP"},"outputs":[],"source":["# посмотрим на изначальное распределение вознаграждения\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n","\n","plt.hist(sample_rewards, bins=20)\n","plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n","plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"hUDOpKxHuQKS"},"source":["### Реализация метода crossentropy  \n","\n","Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGIweAi5uQKT"},"outputs":[],"source":["def select_elites(states_batch, actions_batch, \n","                  rewards_batch, percentile=50):\n","    \"\"\"\n","    Выбирает состояния и действия с заданным перцентилем (rewards >= percentile)\n","    :param states_batch: list of lists of states, states_batch[session_i][t]\n","    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n","    :param rewards_batch: list of rewards, rewards_batch[session_i]\n","    \n","    :returns: elite_states, elite_actions - одномерные \n","    списки состояния и действия, выбранных сессий\n","    \"\"\"\n","    # нужно найти порог вознаграждения по процентилю\n","    # reward_threshold =\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    \n","    # в соответствии с найденным порогом - отобрать \n","    # подходящие состояния и действия\n","    # elite_states = \n","    # elite_actions = \n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    return elite_states, elite_actions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q34NZBHHuQKW"},"outputs":[],"source":["states_batch = [\n","    [1, 2, 3],     # game1\n","    [4, 2, 0, 2],  # game2\n","    [3, 1],        # game3\n","]\n","\n","actions_batch = [\n","    [0, 2, 4],     # game1\n","    [3, 2, 0, 1],  # game2\n","    [3, 3],        # game3\n","]\n","rewards_batch = [\n","    3,  # game1\n","    4,  # game2\n","    5,  # game3\n","]\n","\n","test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n","test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n","test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n","test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n","\n","assert np.all(\n","    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n","       and np.all(\n","    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n","    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n","    \"и действия в хронологическом порядке\"\n","\n","assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1])\\\n","   and np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n","    \"Для процентиля 30 необходимо выбрать \" \\\n","    \"состояния/действия из [3:]\"\n","assert np.all(test_result_90[0] == [3, 1]) and \\\n","       np.all(test_result_90[1] == [3, 3]), \\\n","    \"Для процентиля 90 необходимо выбрать состояния \" \\\n","    \"действия одной игры\"\n","assert np.all(test_result_100[0] == [3, 1]) and \\\n","       np.all(test_result_100[1] == [3, 3]), \\\n","    \"Проверьте использование знаков: >=,  >. \" \\\n","    \"Также проверьте расчет процентиля\"\n","print(\"Тесты пройдены!\")\n"]},{"cell_type":"markdown","metadata":{"id":"ZZzLzP7PuQKb"},"source":["Теперь мы хотим написать обновляющуюся стратегию:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PecfT_xEuQKc"},"outputs":[],"source":["def update_policy(elite_states,elite_actions):\n","    \"\"\"\n","    Новой стратегией будет:\n","    policy[s_i,a_i] ~ #[вхождения  si/ai в лучшие states/actions]\n","    \n","    Не забудьте про нормализацию состояний.\n","    Если какое-то состояние не было посещено, \n","    то используйте равномерное распределение 1./n_actions\n","    \n","    :param elite_states:  список состояний\n","    :param elite_actions: список действий\n","    \"\"\"\n","    new_policy = np.zeros([n_states,n_actions])\n","    for state in range(n_states):\n","        # обновляем стратегию - нормируем новые частоты \n","        # действий и не забываем про непосещенные состояния\n","        # new_policy[state, a] =         \n","        ####### Здесь ваш код ########\n","        raise NotImplementedError\n","        ##############################\n","    return new_policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsCFXHMRuQKf"},"outputs":[],"source":["elite_states, elite_actions = (\n","    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n","    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n","\n","new_policy = update_policy(elite_states, elite_actions)\n","\n","assert np.isfinite(\n","    new_policy).all(), \"Стратегия не должна содержать \" \\\n","                       \"NaNs или +-inf. Проверьте \" \\\n","                       \"деление на ноль. \"\n","assert np.all(\n","    new_policy >= 0), \"Стратегия не должна содержать \" \\\n","                      \"отрицательных вероятностей \"\n","assert np.allclose(new_policy.sum(axis=-1),\n","                   1), \"Суммарная\\ вероятность действий\"\\\n","                       \"для состояния должна равняться 1\"\n","reference_answer = np.array([\n","    [1., 0., 0., 0., 0.],\n","    [0.5, 0., 0., 0.5, 0.],\n","    [0., 0.33333333, 0.66666667, 0., 0.],\n","    [0., 0., 0., 0.5, 0.5]])\n","assert np.allclose(new_policy[:4, :5], reference_answer)\n","print(\"Тесты пройдены!\")"]},{"cell_type":"markdown","metadata":{"id":"VYRLBwtvuQKi"},"source":["### Цикл обучения\n","\n","Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmBnonZCuQKj"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n","    \"\"\"\n","    Удобная функция, для визуализации результатов.\n","    \"\"\"\n","\n","    mean_reward = np.mean(rewards_batch)\n","    threshold = np.percentile(rewards_batch, percentile)\n","    log.append([mean_reward, threshold])\n","    \n","    plt.figure(figsize=[8, 4])\n","    plt.subplot(1, 2, 1)\n","    plt.plot(list(zip(*log))[0], label='Mean rewards')\n","    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.hist(rewards_batch, range=reward_range)\n","    plt.vlines([np.percentile(rewards_batch, percentile)],\n","               [0], [100], label=\"percentile\", color='red')\n","    plt.legend()\n","    plt.grid()\n","    clear_output(True)\n","    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXhGLvjPuQKm"},"outputs":[],"source":["# инициализируем стратегию\n","policy = initialize_policy(n_states, n_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYncUOBfuQKo"},"outputs":[],"source":["n_sessions = 250  # количество сессий для сэмплирования\n","percentile = 50  # перцентиль \n","learning_rate = 0.5 # то как быстро стратегия будет обновляться \n","\n","log = []\n","\n","for i in range(100):\n","    # генерируем n_sessions сессий\n","    # time sessions = []\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    states_batch,actions_batch,rewards_batch = zip(*sessions)\n","    # отбираем лучшие действия и состояния ###\n","    # elite_states, elite_actions = \n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    # обновляем стратегию\n","    # new_policy =\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n","\n","    # display results on chart\n","    show_progress(rewards_batch, log, percentile)"]},{"cell_type":"markdown","metadata":{"id":"3xIsESNpuQKr"},"source":["### Посмотрим на результаты\n","Задача такси быстро сходится, начиная с вознаграждения -1000 к почти оптимальному значению, а потом опять падает до -50/-100. Это вызвано случайностью в самом окружении $-$ случайное начальное состояние пассажира и такси, в начале каждого эпизода. \n","\n","В случае если алгоритм CEM не сможет научиться тому, как решить задачу из какого-то стартового положения, он просто отбросит этот эпизод, т.к. не будет сессий, которые переведут этот эпизод в топ лучших. \n","\n","Для решения этой проблемы можно уменьшить threshold (порог лучших состояний) или изменить способ оценки стратегии, используя новую стратегию, полученную из каждого начального состояния и действия (теоретически правильный способ)."]},{"cell_type":"markdown","metadata":{"id":"_-nMGpEfuQKs"},"source":["## 3. Deep CEM\n","\n","В данной части мы рассмотрим применение CEM вместе с нейронной сетью.\n","Будем обучать многослойную нейронную сеть для решения простой задачи с непрерывным пространством действий.\n","\n","<img src=\"https://camo.githubusercontent.com/8f39c7f54a7798e7f80c9ec5c0bb610696e5c5b7/68747470733a2f2f7469702e64756b652e6564752f696e646570656e64656e745f6c6561726e696e672f677265656b2f6c6573736f6e2f64696767696e675f6465657065725f66696e616c2e6a7067\">"]},{"cell_type":"markdown","metadata":{"id":"sWFuA1hWuQKs"},"source":["Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным пространством состояний.\n","https://gym.openai.com/envs/CartPole-v0/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZhZahUGuQKt"},"outputs":[],"source":["env = gym.make(\"CartPole-v0\")\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape[0]\n","                                        \n","print(f\"состояний: {state_dim} действий: {n_actions}\")"]},{"cell_type":"markdown","metadata":{"id":"UgRpAtU-uQKw"},"source":["### Стратегия с нейронной сетью\n","\n","Попробуем заменить метод обновления вероятностей на нейронную сеть. \n","Будем пользоваться упрощенной реализацией нейронной сети из пакета Scikit-learn.\n","Нам потребуется: \n","* agent.partial_fit(states, actions) - делает один проход обучения по данным. Максимизирует вероятность :actions: из :states:\n","* agent.predict_proba(states) - предсказыает вероятность каждого из действий, в виде матрицы размера [len(states), n_actions]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fOPMKcYuQKx"},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","\n","agent = MLPClassifier(\n","    hidden_layer_sizes=(20, 20),\n","    activation='tanh',\n",")\n","\n","# инициализируем агента под заданное пространство состояний и действий\n","agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9W-5nV3uQK0"},"outputs":[],"source":["def generate_session(env, agent, t_max=1000):\n","    \n","    states,actions = [],[]\n","    total_reward = 0\n","    \n","    s = env.reset()\n","    \n","    for t in range(t_max):\n","        # предсказываем вероятности действий по сети и \n","        # выбираем одно действие\n","        # probs = \n","        # a = \n","        ####### Здесь ваш код ########\n","        raise NotImplementedError\n","        ##############################\n","        \n","        new_s,r,done,info = env.step(a)\n","        \n","        states.append(s)\n","        actions.append(a)\n","        total_reward+=r\n","        \n","        s = new_s\n","        if done: break\n","    return states,actions,total_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pntOSnguQK2"},"outputs":[],"source":["dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n","print(\"состояния:\", np.stack(dummy_states))\n","print(\"действия:\", dummy_actions)\n","print(\"вознаграждение:\", dummy_reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVDwmDFAuQK4"},"outputs":[],"source":["n_sessions = 200\n","percentile = 50\n","log = []\n","\n","for i in range(100):\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n","    \n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    # обновляем стратегию, для предсказания лучших состояний\n","    # elite_actions(y) из elite_states(X)\n","    ####### Здесь ваш код ########\n","    raise NotImplementedError\n","    ##############################\n","    \n","    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n","\n","    if np.mean(rewards_batch) > 190:\n","        print(\"Принято!\")\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAtMSsbN64of"},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yisno0H264of","scrolled":true},"outputs":[],"source":["env = gym.make(\"CartPole-v0\")\n","env = gym.wrappers.Monitor(env, \"./video\", force=True)\n","\n","generate_session(env, agent)\n","\n","env.close()\n","show_video()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3l0yczlx64of"},"outputs":[],"source":["\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Seminar-01-source-task.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
