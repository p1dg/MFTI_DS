{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7sfWBeBq8Wx"
      },
      "source": [
        "## DQN\n",
        "\n",
        "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9txpm2ZBSpWk"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rsaCPHqSpWl",
        "outputId": "fdb224f7-ac99-4239-fe81-b952b3d0ec66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kuderov/mambaforge/envs/rl-mipt/lib/python3.9/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/Users/kuderov/mambaforge/envs/rl-mipt/lib/python3.9/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV22Environment-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/Users/kuderov/mambaforge/envs/rl-mipt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgjQwIsSpWl"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRnOxiAZrOFN",
        "outputId": "3d46b8f1-1a62-4a82-dffa-cfaef68ba23f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.04347211,  0.01259036, -0.01006638, -0.01259871], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbIV7w42Fp1"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HyNGNDSpWm"
      },
      "source": [
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] \\\\\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFkc4eN16Lh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNqTerAgSpWm",
        "outputId": "a263252e-924e-49b8-d569-e5a8231e065e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action_space: 2 \n",
            "State_space: (4,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "print(f'Action_space: {n_actions} \\nState_space: {env.observation_space.shape}')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ihv7jcsSpWn"
      },
      "source": [
        "Задавайте небольшой размер скрытых слоев, например не больше 200.\n",
        "Определяем граф вычислений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3JPKemCSpWn"
      },
      "outputs": [],
      "source": [
        "# TODO: refactor hidden_dims and make it more clear (typing and so on)\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU(),\n",
        "    #    ...\n",
        "    # )\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dims[0]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[1], output_dim),\n",
        "    )\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbzigcFfSpWn"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(network, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = network(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    if epsilon < np.random.random():\n",
        "        action = np.argmax(Q_s)\n",
        "    else:\n",
        "        n_actions = Q_s.shape[-1]\n",
        "        action = np.random.choice(n_actions)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN5c4jwoSpWn"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(\n",
        "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
        "\n",
        "\n",
        "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
        "\n",
        "    # получаем значения q для всех действий из текущих состояний\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # получаем q-values для выбранных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
        "    # predicted_next_qvalues =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    predicted_next_qvalues = network(next_states)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
        "    # next_state_values =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    next_state_values = torch.max(predicted_next_qvalues.detach(), axis=-1)[0]\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # вычисляем target q-values для функции потерь\n",
        "    #  target_qvalues_for_actions =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # для последнего действия в эпизоде используем\n",
        "    # упрощенную формулу Q(s,a) = r(s,a),\n",
        "    # т.к. s' для него не существует\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    loss = torch.mean(losses)\n",
        "    # добавляем регуляризацию на значения Q\n",
        "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_g2MOsSpWn"
      },
      "source": [
        "## Simple DQN\n",
        "\n",
        "Немного модифицированная версия кода, запускающего обучение Q-learning из прошлой тетрадки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5K15RZvSpWn"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooQJI-IrSpWn"
      },
      "outputs": [],
      "source": [
        "def test_dqn():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nZ86IJVSpWo",
        "outputId": "eb63d0d5-7a28-4b5a-e918-1c87440686a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2t/25q4pck13sxbx15kwv4mxx240000gq/T/ipykernel_8220/3266066276.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1673797382507/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
            "  next_states = torch.tensor(next_states, dtype=torch.float32) # shape: [batch_size, state_size]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.000\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.333\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.250\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 9.600\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 9.600\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 10.000\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 10.600\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 10.800\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 13.000\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 14.000\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 15.200\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 16.400\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 19.200\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 20.000\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 20.800\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 31.200\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 38.400\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 51.200\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 49.000\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 92.400\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 128.600\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 143.800\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 165.000\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 165.000\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 148.400\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 158.400\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 173.200\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 160.800\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 163.400\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 137.200\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 84.000\tepsilon = 0.020\n",
            "Epoch: #1649\tmean reward = 48.400\tepsilon = 0.018\n",
            "Epoch: #1699\tmean reward = 25.400\tepsilon = 0.017\n",
            "Epoch: #1749\tmean reward = 22.000\tepsilon = 0.015\n",
            "Epoch: #1799\tmean reward = 34.400\tepsilon = 0.014\n",
            "Epoch: #1849\tmean reward = 31.400\tepsilon = 0.012\n",
            "Epoch: #1899\tmean reward = 39.000\tepsilon = 0.011\n",
            "Epoch: #1949\tmean reward = 51.800\tepsilon = 0.010\n",
            "Epoch: #1999\tmean reward = 68.800\tepsilon = 0.009\n",
            "Epoch: #2049\tmean reward = 73.400\tepsilon = 0.008\n",
            "Epoch: #2099\tmean reward = 88.600\tepsilon = 0.007\n",
            "Epoch: #2149\tmean reward = 127.600\tepsilon = 0.007\n",
            "Epoch: #2199\tmean reward = 136.800\tepsilon = 0.006\n",
            "Epoch: #2249\tmean reward = 121.000\tepsilon = 0.006\n",
            "Epoch: #2299\tmean reward = 159.800\tepsilon = 0.005\n",
            "Epoch: #2349\tmean reward = 145.000\tepsilon = 0.005\n",
            "Epoch: #2399\tmean reward = 101.600\tepsilon = 0.004\n",
            "Epoch: #2449\tmean reward = 83.600\tepsilon = 0.004\n",
            "Epoch: #2499\tmean reward = 86.000\tepsilon = 0.003\n",
            "Epoch: #2549\tmean reward = 32.600\tepsilon = 0.003\n",
            "Epoch: #2599\tmean reward = 37.000\tepsilon = 0.003\n",
            "Epoch: #2649\tmean reward = 38.600\tepsilon = 0.002\n",
            "Epoch: #2699\tmean reward = 45.600\tepsilon = 0.002\n",
            "Epoch: #2749\tmean reward = 43.400\tepsilon = 0.002\n",
            "Epoch: #2799\tmean reward = 48.600\tepsilon = 0.002\n",
            "Epoch: #2849\tmean reward = 48.800\tepsilon = 0.002\n",
            "Epoch: #2899\tmean reward = 54.000\tepsilon = 0.002\n",
            "Epoch: #2949\tmean reward = 67.400\tepsilon = 0.001\n",
            "Epoch: #2999\tmean reward = 123.800\tepsilon = 0.001\n",
            "Epoch: #3049\tmean reward = 146.800\tepsilon = 0.001\n",
            "Epoch: #3099\tmean reward = 174.000\tepsilon = 0.001\n",
            "Epoch: #3149\tmean reward = 202.200\tepsilon = 0.001\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHHO_2xSpWo"
      },
      "source": [
        "## DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyhisdP3SpWo"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN_h8192SpWo"
      },
      "outputs": [],
      "source": [
        "def generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if replay_buffer and glob_step % train_schedule == 0:\n",
        "                # sample new batch: train_batch = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r34ncGGYSpWo"
      },
      "source": [
        "После проверки скорости обучения можете поэкспериментировать с различными `train_schedule`, `batch_size`, а также с размером буфера `replay_buffer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-oAOXalSpWo"
      },
      "outputs": [],
      "source": [
        "def test_dqn_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jpkpTlXSpWp",
        "outputId": "945aff7f-8885-4723-ee2d-cc48faa1826a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.000\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 8.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.000\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 9.800\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 10.000\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 12.200\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 17.400\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 18.400\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 20.000\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 26.800\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 36.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 37.600\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 59.400\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 83.800\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 100.000\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 121.800\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 146.400\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 129.800\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 133.000\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 168.000\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 135.200\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 105.000\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 100.000\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 128.200\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 128.200\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 186.000\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 243.200\tepsilon = 0.030\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_replay_buffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hP5L_ChSpWp"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому переходу, хранящемуся в памяти, значение приоритета. Популярным вариантом является абсолютное значение TD-ошибки.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз - накладно. Приходится искать баланс между точностью и скоростью.\n",
        "\n",
        "Здесь мы будем делать следующее:\n",
        "\n",
        "- использовать TD-ошибку в кач-ве приоритета\n",
        "- после использования батча при обучении, обновляем значения приоритета для этого батча в памяти\n",
        "- будем периодически сортировать память для того, чтобы новые переходы заменяли собой те переходы, у которых наименьшие значения ошибки (т.е. наименьший приоритет)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq7TgsnLSpWp"
      },
      "outputs": [],
      "source": [
        "def softmax(xs, temp=1000.):\n",
        "    if not isinstance(xs, np.ndarray):\n",
        "        xs = np.array(xs)\n",
        "\n",
        "    # Обрати внимание, насколько большая температура по умолчанию!\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    losses = [sample[0] for sample in replay_buffer]\n",
        "    probs = softmax(losses)\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples, p=probs)\n",
        "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        _, s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        dones.append(done)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_losses):\n",
        "    \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "    states, actions, rewards, next_states, is_done = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i]\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning\n",
        "    ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujAVrSilSpWp"
      },
      "outputs": [],
      "source": [
        "def generate_session_prioritized_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # Compute new sample loss (it's the second returning value - `losses` - from compute_td_loss)\n",
        "            # we need `losses.numpy()[0]`\n",
        "            with torch.no_grad():\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                _, losses = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "            replay_buffer.append((losses.numpy()[0], s, a, r, next_s, terminated and not truncated))\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % train_schedule == 0:\n",
        "                # sample new batch: train_batch, indices = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                train_batch, indices = sample_prioritized_batch(replay_buffer, batch_size)\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # compute updated losses for the training batch and update batch in replay buffer\n",
        "                    \"\"\"<codehere>\"\"\"\n",
        "                    _, losses = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                    update_batch(replay_buffer, indices, train_batch, losses.numpy())\n",
        "                    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # periodically re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "            # that have the least loss\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % 25*train_schedule == 0:\n",
        "                replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLEirszhSpWq"
      },
      "outputs": [],
      "source": [
        "def test_dqn_prioritized_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_prioritized_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_prioritized_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7leQWfTwSpWq",
        "outputId": "1f196538-fafe-420d-c71c-9a5343efbcb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.750\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 10.200\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 10.000\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 13.000\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 15.200\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 19.000\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 21.000\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 36.400\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 45.200\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 66.200\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 120.400\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 120.800\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 115.200\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 113.400\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 101.000\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 56.600\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 65.400\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 68.600\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 82.000\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 86.200\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 90.600\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 98.400\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 111.600\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 146.200\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 149.200\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 144.000\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 136.200\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 128.000\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 83.800\tepsilon = 0.020\n",
            "Epoch: #1649\tmean reward = 98.200\tepsilon = 0.018\n",
            "Epoch: #1699\tmean reward = 119.200\tepsilon = 0.017\n",
            "Epoch: #1749\tmean reward = 165.600\tepsilon = 0.015\n",
            "Epoch: #1799\tmean reward = 170.200\tepsilon = 0.014\n",
            "Epoch: #1849\tmean reward = 177.400\tepsilon = 0.012\n",
            "Epoch: #1899\tmean reward = 182.400\tepsilon = 0.011\n",
            "Epoch: #1949\tmean reward = 194.600\tepsilon = 0.010\n",
            "Epoch: #1999\tmean reward = 181.400\tepsilon = 0.009\n",
            "Epoch: #2049\tmean reward = 217.200\tepsilon = 0.008\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_prioritized_replay_buffer()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}