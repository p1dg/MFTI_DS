{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7sfWBeBq8Wx"
      },
      "source": [
        "## DQN\n",
        "\n",
        "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9txpm2ZBSpWk"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6rsaCPHqSpWl",
        "outputId": "fdb224f7-ac99-4239-fe81-b952b3d0ec66"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgjQwIsSpWl"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YRnOxiAZrOFN",
        "outputId": "3d46b8f1-1a62-4a82-dffa-cfaef68ba23f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.04491843, -0.0056384 ,  0.00669168,  0.00082022], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbIV7w42Fp1"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "Для начала попробуйте использовать только полносвязные слои (`torch.nn.Linear`) и простые активационные функции (`torch.nn.ReLU`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HyNGNDSpWm"
      },
      "source": [
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] \\\\\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "\n",
        "где\n",
        "\n",
        "- $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "- $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5BFkc4eN16Lh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pNqTerAgSpWm",
        "outputId": "a263252e-924e-49b8-d569-e5a8231e065e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action_space: 2 \n",
            "State_space: (4,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "print(f'Action_space: {n_actions} \\nState_space: {env.observation_space.shape}')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ihv7jcsSpWn"
      },
      "source": [
        "Задавайте небольшой размер скрытых слоев, например не больше 200.\n",
        "Определяем граф вычислений:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E3JPKemCSpWn"
      },
      "outputs": [],
      "source": [
        "# TODO: refactor hidden_dims and make it more clear (typing and so on)\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU(),\n",
        "    #    ...\n",
        "    # )\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dims[0]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[1], output_dim),\n",
        "    )\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QbzigcFfSpWn"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(network, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = network(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    if epsilon < np.random.random():\n",
        "        action = np.argmax(Q_s)\n",
        "    else:\n",
        "        n_actions = Q_s.shape[-1]\n",
        "        action = np.random.choice(n_actions)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cN5c4jwoSpWn"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(\n",
        "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
        "\n",
        "\n",
        "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
        "\n",
        "    # получаем значения q для всех действий из текущих состояний\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # получаем q-values для выбранных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
        "    # predicted_next_qvalues =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    predicted_next_qvalues = network(next_states)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
        "    # next_state_values =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    next_state_values = torch.max(predicted_next_qvalues.detach(), axis=-1)[0]\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # вычисляем target q-values для функции потерь\n",
        "    #  target_qvalues_for_actions =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # для последнего действия в эпизоде используем\n",
        "    # упрощенную формулу Q(s,a) = r(s,a),\n",
        "    # т.к. s' для него не существует\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    loss = torch.mean(losses)\n",
        "    # добавляем регуляризацию на значения Q\n",
        "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_g2MOsSpWn"
      },
      "source": [
        "## Simple DQN\n",
        "\n",
        "Немного модифицированная версия кода, запускающего обучение Q-learning из прошлой тетрадки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_5K15RZvSpWn"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ooQJI-IrSpWn"
      },
      "outputs": [],
      "source": [
        "def test_dqn():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 1000, 50# 10000, 50\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2nZ86IJVSpWo",
        "outputId": "eb63d0d5-7a28-4b5a-e918-1c87440686a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 8.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 8.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 8.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 24.250\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 21.200\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 21.600\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 21.600\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 22.000\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 9.400\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 10.400\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 11.000\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 13.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 15.600\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 18.600\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 23.200\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 26.400\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 30.000\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 27.200\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 32.600\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 35.400\tepsilon = 0.068\n"
          ]
        }
      ],
      "source": [
        "test_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHHO_2xSpWo"
      },
      "source": [
        "## DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QyhisdP3SpWo"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples)\n",
        "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        dones.append(done)\n",
        "\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cN_h8192SpWo"
      },
      "outputs": [],
      "source": [
        "def generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if replay_buffer and glob_step % train_schedule == 0:\n",
        "                # sample new batch: train_batch = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                train_batch = sample_batch(replay_buffer, batch_size)\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r34ncGGYSpWo"
      },
      "source": [
        "После проверки скорости обучения можете поэкспериментировать с различными `train_schedule`, `batch_size`, а также с размером буфера `replay_buffer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o-oAOXalSpWo"
      },
      "outputs": [],
      "source": [
        "def test_dqn_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 1000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2jpkpTlXSpWp",
        "outputId": "945aff7f-8885-4723-ee2d-cc48faa1826a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 10.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.500\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 9.600\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 9.400\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 9.400\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 10.400\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 12.000\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 20.600\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 25.200\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 37.200\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 55.400\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 61.000\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 60.600\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 114.200\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 145.400\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 149.200\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 146.000\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 137.600\tepsilon = 0.068\n"
          ]
        }
      ],
      "source": [
        "test_dqn_replay_buffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hP5L_ChSpWp"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому переходу, хранящемуся в памяти, значение приоритета. Популярным вариантом является абсолютное значение TD-ошибки.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз - накладно. Приходится искать баланс между точностью и скоростью.\n",
        "\n",
        "Здесь мы будем делать следующее:\n",
        "\n",
        "- использовать TD-ошибку в кач-ве приоритета\n",
        "- после использования батча при обучении, обновляем значения приоритета для этого батча в памяти\n",
        "- будем периодически сортировать память для того, чтобы новые переходы заменяли собой те переходы, у которых наименьшие значения ошибки (т.е. наименьший приоритет)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zq7TgsnLSpWp"
      },
      "outputs": [],
      "source": [
        "def softmax(xs, temp=1000.):\n",
        "    if not isinstance(xs, np.ndarray):\n",
        "        xs = np.array(xs)\n",
        "\n",
        "    # Обрати внимание, насколько большая температура по умолчанию!\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    losses = [sample[0] for sample in replay_buffer]\n",
        "    probs = softmax(losses)\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples, p=probs)\n",
        "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        _, s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        dones.append(done)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_losses):\n",
        "    \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "    states, actions, rewards, next_states, is_done = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i]\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning\n",
        "    ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ujAVrSilSpWp"
      },
      "outputs": [],
      "source": [
        "def generate_session_prioritized_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # Compute new sample loss (it's the second returning value - `losses` - from compute_td_loss)\n",
        "            # we need `losses.numpy()[0]`\n",
        "            with torch.no_grad():\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                _, losses = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "            replay_buffer.append((losses.numpy()[0], s, a, r, next_s, terminated and not truncated))\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % train_schedule == 0:\n",
        "                # sample new batch: train_batch, indices = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                train_batch, indices = sample_prioritized_batch(replay_buffer, batch_size)\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # compute updated losses for the training batch and update batch in replay buffer\n",
        "                    \"\"\"<codehere>\"\"\"\n",
        "                    _, losses = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                    update_batch(replay_buffer, indices, train_batch, losses.numpy())\n",
        "                    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # periodically re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "            # that have the least loss\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % 25*train_schedule == 0:\n",
        "                replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lLEirszhSpWq"
      },
      "outputs": [],
      "source": [
        "def test_dqn_prioritized_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 1000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_prioritized_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_prioritized_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7leQWfTwSpWq",
        "outputId": "1f196538-fafe-420d-c71c-9a5343efbcb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 10.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.250\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 9.600\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 9.200\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 9.400\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 11.800\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 11.800\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 16.400\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 38.600\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 48.600\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 63.000\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 74.800\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 128.000\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 122.200\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 141.000\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 182.200\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 202.000\tepsilon = 0.075\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_prioritized_replay_buffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
