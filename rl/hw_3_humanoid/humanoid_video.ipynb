{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_actor = 0.0003\n",
    "lr_critic = 0.0003\n",
    "Iter = 15000\n",
    "MAX_STEP = 10000\n",
    "gamma = 0.98\n",
    "lambd = 0.98\n",
    "batch_size = 64\n",
    "epsilon = 0.2\n",
    "l2_rate = 0.001\n",
    "beta = 3\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, N_S, N_A):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_S, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.sigma = nn.Linear(64, N_A)\n",
    "        self.mu = nn.Linear(64, N_A)\n",
    "        self.mu.weight.data.mul_(0.1)\n",
    "        self.mu.bias.data.mul_(0.0)\n",
    "        # self.set_init([self.fc1,self.fc2, self.mu, self.sigma])\n",
    "        self.distribution = torch.distributions.Normal\n",
    "\n",
    "    def set_init(self, layers):\n",
    "        for layer in layers:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=0.1)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = torch.tanh(self.fc1(s))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        mu = self.mu(x)\n",
    "        log_sigma = self.sigma(x)\n",
    "        # log_sigma = torch.zeros_like(mu)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return mu, sigma\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        mu, sigma = self.forward(s)\n",
    "        Pi = self.distribution(mu, sigma)\n",
    "        return Pi.sample().cpu().numpy()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, N_S):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_S, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.fc3.weight.data.mul_(0.1)\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "        # self.set_init([self.fc1, self.fc2, self.fc2])\n",
    "\n",
    "    def set_init(self, layers):\n",
    "        for layer in layers:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=0.1)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = torch.tanh(self.fc1(s))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        values = self.fc3(x)\n",
    "        return values\n",
    "\n",
    "\n",
    "class Ppo:\n",
    "    def __init__(self, N_S, N_A):\n",
    "        self.actor_net = Actor(N_S, N_A).to(device)\n",
    "        self.critic_net = Critic(N_S).to(device)\n",
    "        self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=lr_actor)\n",
    "        self.critic_optim = optim.Adam(\n",
    "            self.critic_net.parameters(), lr=lr_critic, weight_decay=l2_rate\n",
    "        )\n",
    "        self.critic_loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    def train(self, memory):\n",
    "        # memory = np.array(memory)\n",
    "        states = torch.tensor(\n",
    "            np.vstack(np.array([val[0] for val in memory])), dtype=torch.float32\n",
    "        ).to(device)\n",
    "\n",
    "        actions = torch.tensor(\n",
    "            np.array([val[1] for val in memory]), dtype=torch.float32\n",
    "        ).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            np.array([val[2] for val in memory]), dtype=torch.float32\n",
    "        ).to(device)\n",
    "        masks = torch.tensor(\n",
    "            np.array([val[3] for val in memory]), dtype=torch.float32\n",
    "        ).to(device)\n",
    "\n",
    "        values = self.critic_net(states)\n",
    "\n",
    "        returns, advants = self.get_gae(rewards, masks, values)\n",
    "        old_mu, old_std = self.actor_net(states)\n",
    "        pi = self.actor_net.distribution(old_mu, old_std)\n",
    "\n",
    "        old_log_prob = pi.log_prob(actions).sum(1, keepdim=True)\n",
    "\n",
    "        n = len(states)\n",
    "        arr = np.arange(n)\n",
    "        for epoch in range(1):\n",
    "            np.random.shuffle(arr)\n",
    "            for i in range(n // batch_size):\n",
    "                b_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "                b_states = states[b_index]\n",
    "                b_advants = advants[b_index].unsqueeze(1)\n",
    "                b_actions = actions[b_index]\n",
    "                b_returns = returns[b_index].unsqueeze(1)\n",
    "\n",
    "                mu, std = self.actor_net(b_states)\n",
    "                pi = self.actor_net.distribution(mu, std)\n",
    "                new_prob = pi.log_prob(b_actions).sum(1, keepdim=True)\n",
    "                old_prob = old_log_prob[b_index].detach()\n",
    "                # KL_penalty = self.kl_divergence(old_mu[b_index],old_std[b_index],mu,std)\n",
    "                ratio = torch.exp(new_prob - old_prob)\n",
    "\n",
    "                surrogate_loss = ratio * b_advants\n",
    "                values = self.critic_net(b_states)\n",
    "\n",
    "                critic_loss = self.critic_loss_func(values, b_returns)\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "\n",
    "                ratio = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "                clipped_loss = ratio * b_advants\n",
    "\n",
    "                actor_loss = -torch.min(surrogate_loss, clipped_loss).mean()\n",
    "                # actor_loss = -(surrogate_loss-beta*KL_penalty).mean()\n",
    "\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "\n",
    "                self.actor_optim.step()\n",
    "\n",
    "    def kl_divergence(self, old_mu, old_sigma, mu, sigma):\n",
    "        old_mu = old_mu.detach()\n",
    "        old_sigma = old_sigma.detach()\n",
    "\n",
    "        kl = (\n",
    "            torch.log(old_sigma)\n",
    "            - torch.log(sigma)\n",
    "            + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / (2.0 * sigma.pow(2))\n",
    "            - 0.5\n",
    "        )\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    def get_gae(self, rewards, masks, values):\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        advants = torch.zeros_like(rewards)\n",
    "        running_returns = 0\n",
    "        previous_value = 0\n",
    "        running_advants = 0\n",
    "\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
    "            running_tderror = (\n",
    "                rewards[t] + gamma * previous_value * masks[t] - values.data[t]\n",
    "            )\n",
    "            running_advants = (\n",
    "                running_tderror + gamma * lambd * running_advants * masks[t]\n",
    "            )\n",
    "\n",
    "            returns[t] = running_returns\n",
    "            previous_value = values.data[t]\n",
    "            advants[t] = running_advants\n",
    "        advants = (advants - advants.mean()) / advants.std()\n",
    "        return returns, advants\n",
    "\n",
    "\n",
    "class Nomalize:\n",
    "    def __init__(self, N_S):\n",
    "        self.mean = np.zeros((N_S,))\n",
    "        self.std = np.zeros((N_S,))\n",
    "        self.stdd = np.zeros((N_S,))\n",
    "        self.n = 0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = x\n",
    "        else:\n",
    "            old_mean = self.mean.copy()\n",
    "            self.mean = old_mean + (x - old_mean) / self.n\n",
    "            self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
    "        if self.n > 1:\n",
    "            self.std = np.sqrt(self.stdd / (self.n - 1))\n",
    "        else:\n",
    "            self.std = self.mean\n",
    "\n",
    "        x = x - self.mean\n",
    "        x = x / (self.std + 1e-8)\n",
    "        x = np.clip(x, -5, +5)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"rgb_array\")\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "ppo = Ppo(N_S, N_A)\n",
    "\n",
    "ppo.actor_net.load_state_dict(torch.load(\"actor_net.pth\"))\n",
    "ppo.critic_net.load_state_dict(torch.load(\"critic_net.pth\"))\n",
    "\n",
    "out = imageio.get_writer(\"humanoid_video.mp4\", fps=30)\n",
    "\n",
    "nomalize = Nomalize(N_S)\n",
    "s = nomalize(env.reset()[0])\n",
    "score = 0\n",
    "done = False\n",
    "frames = []\n",
    "while not done:\n",
    "    a = ppo.actor_net.choose_action(\n",
    "        torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0).to(device)\n",
    "    )[0]\n",
    "    s_, r, terminated, truncated, info = env.step(a)\n",
    "    score += r\n",
    "    s = nomalize(s_)\n",
    "    out.append_data(env.render())\n",
    "\n",
    "    done = terminated or truncated\n",
    "\n",
    "out.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267.0585518925236"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
