lr_actor = 0.0003
lr_critic = 0.0003
Iter = 15000
MAX_STEP = 10000
gamma = 0.98
lambd = 0.98
batch_size = 64
epsilon = 0.2
l2_rate = 0.001
beta = 3

в процессе обучения PPO я использовал параметры приведенные выше - поговорим про каждый из них. 
lr_actor/lr_critic используется в алгоритме град спуска Adam при обучении актора и критика - оптимальные значения 10e-3> - если брать больше то модель начинает скакать и не учится, 10e-4 слишком мало - модель учится гораздо лучше, но делает это оч долго
10e-4*3 - золотая середина по моему мнению 

Iter - число итераций для обучения, модель начинает показывать прирост с самого начала, но на дистанции это видно с 2000 итераций, так что желательно брать выше этого числа (я доучил до 4к)
MAX_STEP - максимальное число шагов в эпизоде - с определенного момента гуманойд начинает шагать так уверенно, что в принципе может прошагать вечно - 10000 верхний порог когда можно быть уверенным, что он идет бесконечно (подбирался экспертно)
gamma и lambd - отвечает за величину running_tderror и running_advants, в целом важно держать их большими, чтобы гуманойд не решил что "выжить здесь и сейчас" любым действием гораздо важнее чем выжить на дистанции, пробовал от 0.95 lj 0.99, в целом ок показались 0.98 и 0.99

batch_size - тут классика, размер батча для обучения, пробовал 32/64/128 - в целом 64 и 128 похожи по импакту на обучение - поэтому сделал выбор в пользу 64 тк он считается быстрее
epsilon - коэфициент отвечающий за величину масштабирования torch.exp(new_prob - old_prob) (аля КЛ-дивергеция) - чем больше, тем хуже - 0.2 лучше всего тк при 0.1 модель перестает учится
l2_rate - регулизатор весов critic,  стандартное значение - больше, перестаем обучаться
beta - де факто не ипользовался, нужен был для лосса с КЛ дивергенцией