{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08b030de",
      "metadata": {
        "id": "08b030de"
      },
      "source": [
        "## Метод Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3a82f3",
      "metadata": {
        "id": "cb3a82f3"
      },
      "source": [
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
        "\n",
        "Встает вопрос, как оценить $Q^\\pi(s, a)$? Ранее в REINFORCE мы использовали отдачу $R_t$ (полученную методом Монте-Карло) в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции - критика.\n",
        "\n",
        "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое семейство различных алгоритмов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### REINFORCE Loss\n",
        "\n",
        "Дано:\n",
        "- $\\pi_\\theta(a|s)$: Функция стратегии, параметризованная $ \\theta $, представляющая вероятность предпринятия действия $a$ в состоянии $ s $.\n",
        "- $R_t$: Возврат (совокупное вознаграждение) начиная с времени $t$.\n",
        "\n",
        "Цель - максимизировать ожидаемый возврат:\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R_t] $$\n",
        "\n",
        "Градиент $ J(\\theta)$ по параметрам стратегии $ \\theta $ есть:\n",
        "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot R_t] $$\n",
        "\n",
        "Функция потерь, которую мы стремимся минимизировать, это отрицательное значение $J(\\theta)$, так что потери $(L)$ могут быть выражены как:\n",
        "$$ L(\\theta) = -\\mathbb{E}_{\\pi_\\theta}[\\log \\pi_\\theta(a|s) \\cdot R_t] $$\n",
        "\n",
        "### Actor-Critic Loss\n",
        "\n",
        "Методы Actor-Critic улучшают подход на основе градиента стратегии, уменьшая дисперсию оценки $ R_t $ с помощью функции критика $ V(s) $ или $Q(s, a)$. Функция потерь в методах Actor-Critic объединяет потери градиента стратегии (потери актера) с потерями функции значения (потери критика).\n",
        "\n",
        "Потери актера (похоже на REINFORCE, но часто использует преимущество $A_t$ вместо $R_t$:\n",
        "$$ L_{actor}(\\theta) = -\\mathbb{E}_{\\pi_\\theta}[\\log \\pi_\\theta(a|s) \\cdot A_t] $$\n",
        "где  $A_t = R_t - V(s)$ это преимущество, указывающее, насколько действие $a$ лучше по сравнению со средним действием в состоянии $s$.\n",
        "\n",
        "Потери критика:\n",
        "$$ L_{critic}(\\phi) = \\mathbb{E}_{\\pi_\\theta}[(R_t - V_\\phi(s))^2] $$\n",
        "где $V_\\phi(s)$ это аппроксимированная функция значения параметрами $\\phi$, оценивающая ожидаемый возврат из состояния $s$.\n",
        "\n",
        "Общие потери для модели Actor-Critic будут комбинацией $L_{actor}$ и $L_{critic}$, часто с дополнительными условиями для регуляризации или бонуса энтропии для поощрения исследования:\n",
        "$$ L_{total} = L_{actor}(\\theta) + \\lambda L_{critic}(\\phi) + \\text{условия энтропии или регуляризации} $$\n",
        "\n",
        "Эта формулировка позволяет обновлениям как стратегии, так и функции значения двигаться в направлении, улучшающем ожидаемый возврат, при этом критик помогает стабилизировать обучение актера, предоставляя базовую линию для производительности стратегии.\n",
        "\n"
      ],
      "metadata": {
        "id": "P6yDIYklBgek"
      },
      "id": "P6yDIYklBgek"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a870514-f49e-4a18-9234-ba682ce1ee07",
      "metadata": {
        "id": "4a870514-f49e-4a18-9234-ba682ce1ee07"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38ba055",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b38ba055",
        "outputId": "dc703c1b-19a9-478c-e187-b49fd04dde8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb481bf0",
      "metadata": {
        "id": "bb481bf0"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cedde6d",
      "metadata": {
        "id": "4cedde6d"
      },
      "outputs": [],
      "source": [
        "def print_mean_reward(step, episode_rewards):\n",
        "    if not episode_rewards:\n",
        "        return\n",
        "\n",
        "    t = min(50, len(episode_rewards))\n",
        "    mean_reward = sum(episode_rewards[-t:]) / t\n",
        "    print(f\"step: {str(step).zfill(6)}, mean reward: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def run(\n",
        "        env: gym.Env, hidden_size: int, lr: float, gamma: float, max_episodes: int,\n",
        "        rollout_size: int, replay_buffer_size: int, critic_batch_size: int, critic_updates_per_actor: int\n",
        "):\n",
        "    # Инициализируйте агента `agent`, когда сделаете саму реализацию агента ниже по заданию.\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    states_dim = env.observation_space.shape[0]\n",
        "    actions_dim = env.action_space.n\n",
        "    agent =\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    step = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i_episode in range(1, max_episodes + 1):\n",
        "        cumulative_reward = 0\n",
        "        terminated = False\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        while not terminated:\n",
        "            step += 1\n",
        "\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            agent.append_to_replay_buffer(state, action, reward, next_state, terminated)\n",
        "            state = next_state\n",
        "            cumulative_reward += reward\n",
        "            terminated |= truncated\n",
        "\n",
        "        episode_rewards.append(cumulative_reward)\n",
        "\n",
        "        # выполняем обновление\n",
        "        if agent.update(rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "            mean_reward = print_mean_reward(step, episode_rewards)\n",
        "            if mean_reward >= 200:\n",
        "                print('Принято!')\n",
        "                return\n",
        "            episode_rewards = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d624df",
      "metadata": {
        "id": "14d624df"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "from operator import attrgetter\n",
        "\n",
        "class ActorBatch:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.q_values = []\n",
        "\n",
        "    def append(self, log_prob, q_value):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.q_values.append(q_value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.logprobs.clear()\n",
        "        self.q_values.clear()\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition', ['loss', 'state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def softmax(self, xs, temp=1000.):\n",
        "        if not isinstance(xs, np.ndarray):\n",
        "            xs = np.array(xs)\n",
        "\n",
        "        # Обрати внимание, насколько большая температура по умолчанию!\n",
        "        exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "        return exp_xs / exp_xs.sum()\n",
        "\n",
        "    def append(self, loss, state, action, reward, next_state, done):\n",
        "        sample = Transition(loss, state, action, reward, next_state, done)\n",
        "        self.buffer.append(sample)\n",
        "\n",
        "    def sample_batch(self, n_samples):\n",
        "        # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "        # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "        # Also, keep samples' indices (into `indices`) to return them too!\n",
        "        losses = [sample.loss for sample in self.buffer]\n",
        "        probs = self.softmax(losses)\n",
        "        indices = np.random.choice(len(self.buffer), n_samples, p=probs)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            _, s, a, r, n_s, done = self.buffer[i]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(n_s)\n",
        "            dones.append(done)\n",
        "\n",
        "        batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "        return batch, indices\n",
        "\n",
        "    def update_batch(self, indices, batch, new_losses):\n",
        "        \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "        states, actions, rewards, next_states, is_done = batch\n",
        "\n",
        "        for i in range(len(indices)):\n",
        "            new_sample = Transition(new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i])\n",
        "            self.buffer[indices[i]] = new_sample\n",
        "\n",
        "    def sort(self):\n",
        "        \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning\n",
        "        ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "        new_rb = deque(maxlen=self.buffer.maxlen)\n",
        "        new_rb.extend(sorted(self.buffer, key=attrgetter('loss')))\n",
        "        self.buffer = new_rb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8b058f",
      "metadata": {
        "id": "ac8b058f"
      },
      "source": [
        "Попробуйте сначала реализовать без памяти прецедентов, а затем дополните вашу реализацию. Текущей реализацией приоритизированной памяти достаточно, чтобы пользоваться ей по аналогии с AgentBatch, стоит лишь добавить метод выборки всех данных по аналогии с `sample_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b2641d",
      "metadata": {
        "id": "62b2641d"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = to_tensor(state)\n",
        "        return self.net(state)\n",
        "\n",
        "\n",
        "class ActorCriticModel(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
        "        # self.net, self.actor_head, self.critic_head =\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "        return action, log_prob, q_value\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        # Вычислите значения Q-функции для данного состояния\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "        return q_values\n",
        "\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_size, lr, gamma, replay_buffer_size):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "        self.actor_batch = ActorBatch()\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(replay_buffer_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
        "        # Не забудьте сделать q_value.detach()\n",
        "        # self.actor_batch.append(..)\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "        return action\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        return self.actor_critic.evaluate(state)\n",
        "\n",
        "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.actor_batch.q_values) < rollout_size:\n",
        "            return False\n",
        "\n",
        "        self.update_actor()\n",
        "        self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "        self.actor_batch.clear()\n",
        "        return True\n",
        "\n",
        "    def update_actor(self):\n",
        "        q_values = to_tensor(self.actor_batch.q_values)\n",
        "        logprobs = torch.stack(self.actor_batch.logprobs).to(device)\n",
        "\n",
        "        # Реализуйте шаг обновления актора. Опционально: сделайте нормализацию отдач\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "        # Нормализация отдач\n",
        "\n",
        "        # Вычислите ошибку `loss` и произведите шаг обновления градиентным спуском\n",
        "        loss =\n",
        "\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    def update_critic(self, batch_size, critic_updates_per_actor):\n",
        "        # Реализуйте critic_updates_per_actor шагов обучения критика.\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        # ограничивает сверху количество эпох для буфера небольшого размера\n",
        "        critic_updates_per_actor = min(\n",
        "            critic_updates_per_actor,\n",
        "            5 * len(self.replay_buffer.buffer) // batch_size\n",
        "        )\n",
        "\n",
        "        for _ in range(critic_updates_per_actor):\n",
        "            # generate batch from replay_buffer\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # compute td loss\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # compute updated losses for the training batch and update batch in replay buffer\n",
        "\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "        # re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "        # that have the least loss\n",
        "        if len(self.replay_buffer.buffer) >= .75 * (self.replay_buffer.buffer.maxlen):\n",
        "            self.replay_buffer.sort()\n",
        "\n",
        "    def append_to_replay_buffer(self, s, a, r, next_s, done):\n",
        "        # Добавьте новый экземпляр данных в память прецедентов.\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # compute td-loss for sample\n",
        "\n",
        "        # add to replay buffer\n",
        "\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    def compute_td_loss(\n",
        "        self, states, actions, rewards, next_states, is_done, check_shapes=False, regularizer=.1\n",
        "    ):\n",
        "        \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch\"\"\"\n",
        "\n",
        "        # переводим входные данные в тензоры\n",
        "        states = to_tensor(states)                      # shape: [batch_size, state_size]\n",
        "        actions = to_tensor(actions, int).long()        # shape: [batch_size]\n",
        "        rewards = to_tensor(rewards)                    # shape: [batch_size]\n",
        "        next_states = to_tensor(next_states)            # shape: [batch_size, state_size]\n",
        "        is_done = to_tensor(is_done, bool)              # shape: [batch_size]\n",
        "\n",
        "        # Реализуйте шаг обновления критика\n",
        "        \"\"\"<codehere>\"\"\"\n",
        "        # получаем значения q для всех действий из текущих состояний\n",
        "        predicted_qvalues = self.evaluate(states)\n",
        "\n",
        "        # получаем q-values для выбранных действий\n",
        "        predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_next_qvalues = self.evaluate(next_states)\n",
        "            next_state_values = torch.max(predicted_next_qvalues, axis=-1)[0]\n",
        "\n",
        "            assert next_state_values.dtype == torch.float32\n",
        "\n",
        "            # вычисляем target q-values для функции потерь\n",
        "            #  target_qvalues_for_actions =\n",
        "            target_qvalues_for_actions = rewards + self.gamma * next_state_values\n",
        "\n",
        "            # для последнего действия в эпизоде используем\n",
        "            # упрощенную формулу Q(s,a) = r(s,a),\n",
        "            # т.к. s' для него не существует\n",
        "            target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "        losses = (predicted_qvalues_for_actions - target_qvalues_for_actions) ** 2\n",
        "\n",
        "        # MSE loss для минимизации\n",
        "        loss = torch.mean(losses)\n",
        "        # добавляем регуляризацию на значения Q\n",
        "        loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "        \"\"\"</codehere>\"\"\"\n",
        "\n",
        "        return loss, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3366c97f",
      "metadata": {
        "id": "3366c97f",
        "outputId": "b7eb635f-9d24-4db6-9517-82e0c0c696fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 000546, mean reward: 21.00\n",
            "step: 001046, mean reward: 9.84\n",
            "step: 001548, mean reward: 10.68\n",
            "step: 002056, mean reward: 15.88\n",
            "step: 002559, mean reward: 17.96\n",
            "step: 003061, mean reward: 10.04\n",
            "step: 003595, mean reward: 76.29\n",
            "step: 004115, mean reward: 47.27\n",
            "step: 004640, mean reward: 105.00\n",
            "step: 005225, mean reward: 146.25\n",
            "step: 005737, mean reward: 20.48\n",
            "step: 006240, mean reward: 16.77\n",
            "step: 006809, mean reward: 189.67\n",
            "step: 007330, mean reward: 104.20\n",
            "step: 007959, mean reward: 125.80\n",
            "step: 008574, mean reward: 153.75\n",
            "step: 009150, mean reward: 192.00\n",
            "step: 009692, mean reward: 135.50\n",
            "step: 010325, mean reward: 126.60\n",
            "step: 010920, mean reward: 119.00\n",
            "step: 011458, mean reward: 107.60\n",
            "step: 012039, mean reward: 116.20\n",
            "step: 012607, mean reward: 142.00\n",
            "step: 013240, mean reward: 316.50\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "run(\n",
        "    env = TimeLimit(gym.make(env_name), 1000),\n",
        "    max_episodes = 50000,  # количество эпизодов обучения\n",
        "    hidden_size = 64,  # кол-во переменных в скрытых слоях\n",
        "    rollout_size = 500,  # через столько шагов стратегия будет обновляться\n",
        "    lr = 0.01, # learning rate\n",
        "    gamma = 0.995,  # дисконтирующий множитель,\n",
        "    replay_buffer_size = 5000,\n",
        "    critic_batch_size = 64,\n",
        "    critic_updates_per_actor = 32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OP5O6v4iLTg8"
      },
      "id": "OP5O6v4iLTg8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Soft Actor-Critic (SAC)\n",
        "\n",
        "Soft Actor-Critic (SAC) — алгоритм обучения с подкреплением, оптимизирующий стохастическую стратегию для максимизации ожидаемого возврата с дополнительным энтропийным бонусом. Этот бонус поощряет исследование, улучшая устойчивость алгоритма и уменьшая его чувствительность к начальным условиям.\n",
        "\n",
        "Основные компоненты SAC включают:\n",
        "- **Стратегию (актер)**, параметризованную $\\pi_\\theta(a|s)$, определяющую вероятностное распределение действий.\n",
        "- **Две функции Q-значения (критики)**, $Q_{\\phi_1}(s, a)$ и $Q_{\\phi_2}(s, a)$, минимизирующие ошибку временной разницы (Temporal Difference, TD).\n",
        "- **Функцию значения (V)**, $V_\\psi(s)$, для оценки ожидаемого возврата из состояния $s$, не зависящего от конкретных действий.\n",
        "\n",
        "#### Функция потерь для SAC:\n",
        "\n",
        "1. **Потери критика**:\n",
        "Критики обновляются для минимизации среднеквадратичной ошибки TD:\n",
        "$$L_{critic}(\\phi_i) = \\mathbb{E}_{(s, a, r, s') \\sim D}\\left[\\left(Q_{\\phi_i}(s, a) - (r + \\gamma (V_{\\psi'}(s') - \\alpha \\log \\pi_\\theta(a|s)))\\right)^2\\right],$$\n",
        "где $\\gamma$ — фактор дисконтирования, $\\alpha$ — параметр температуры, контролирующий важность энтропийного бонуса, и $D$ — опыт из буфера воспроизведения.\n",
        "\n",
        "2. **Потери актера**:\n",
        "Актер обновляется через градиентный подъем, максимизируя оценочное Q-значение минус энтропийный бонус:\n",
        "$$L_{actor}(\\theta) = -\\mathbb{E}_{s \\sim D, a \\sim \\pi_\\theta}\\left[\\min_{i=1,2} Q_{\\phi_i}(s, a) - \\alpha \\log \\pi_\\theta(a|s)\\right].$$\n",
        "\n",
        "3. **Потери функции значения**:\n",
        "Функция значения обновляется для минимизации разницы между её оценками и минимальным Q-значением, скорректированным на энтропийный бонус:\n",
        "$$L_{value}(\\psi) = \\mathbb{E}_{s \\sim D}\\left[\\left(V_\\psi(s) - \\min_{i=1,2} Q_{\\phi_i}(s, a_{\\theta}) + \\alpha \\log \\pi_\\theta(a_{\\theta}|s)\\right)^2\\right],$$\n",
        "где $a_{\\theta}$ — действие, выбранное согласно текущей стратегии.\n",
        "\n",
        "4. **Автоматическая настройка параметра температуры** $\\alpha$:\n",
        "Для автоматической настройки $\\alpha$, используется дополнительная функция потерь, которая обновляет $\\alpha$ для поддержания желаемого уровня энтропии.\n"
      ],
      "metadata": {
        "id": "CC4TQ0QgLToL"
      },
      "id": "CC4TQ0QgLToL"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CenogUlNLWYh"
      },
      "id": "CenogUlNLWYh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}